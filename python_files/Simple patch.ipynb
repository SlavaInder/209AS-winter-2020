{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessaary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets as nets\n",
    "\n",
    "import tempfile\n",
    "from urllib.request import urlretrieve\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import PIL\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup interactive session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tensorflow weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfortunately, while initiaizing the model in tensorflow it is necessary \n",
    "# to specify the input. The reason is how graphs work in tf. Input images\n",
    "# should be scaled to the range of [-1, 1] and have format \n",
    "# (num_images, 299, 299, 3)\n",
    "def init_inception(input_images):\n",
    "    # download default values from the lib in the form of dictionary\n",
    "    # keys include: activation_funcntion    (ReLU)\n",
    "    #               decay                   (0.9997)\n",
    "    #               normilizer              (batch_norm)\n",
    "    #               etc.\n",
    "    # keys you want to change you can type inside (*)\n",
    "    arg_scope = nets.inception.inception_v3_arg_scope(weight_decay=0.0)\n",
    "    \n",
    "    # the following is a context manager. That is, within the space specified\n",
    "    # with tf.contrib.framework.arg_scope(dict) values stored in  the dict are\n",
    "    # automatically passed to all functions. That is, when we declare inception_v3()\n",
    "    # instead of specifying each argument manually in a way inc_v3(decay = [dict['decay']]),\n",
    "    # we have them automatically set.\n",
    "    with tf.contrib.framework.arg_scope(arg_scope):\n",
    "        \n",
    "        # this function creates architecture (not weights) of inception_v3. It returns: \n",
    "        # tf_logits  - a vecotr of raw (not scaled to [0, 1]) scores for all of the classes\n",
    "        #     of the net (i.e. its output tensor) you can apply softmax here to get probabilities\n",
    "        #     or just get amax to find predicted class                                                 \n",
    "        # end_points - a dictionary containing output tensors of important layers of the model\n",
    "        tf_logits, end_points = tf.contrib.slim.nets.inception.inception_v3(input_images,\n",
    "                                                                            num_classes=1001,   # default \n",
    "                                                                            is_training=False)\n",
    "        \n",
    "        # logits is a tensor of shape (?, 1001). We are interested only in teh second \n",
    "        # deimension of the tensor (because first is background class)\n",
    "        logits = logits[:,1:]\n",
    "        \n",
    "        # get probabilities\n",
    "        probs = tf.nn.softmax(logits) \n",
    "    \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input images should be scaled to the range of [-1, 1] and have format (num_images, 299, 299, 3)\n",
    "# this function takes as an input *.jpg image (because *.png-s actually have 4-dims),\n",
    "# crops it to the size 300x300, and convert to tensor. For convenience, it returns both scaled tensor\n",
    "# and preprocessed image.\n",
    "def preprocessing(raw_pillow_image):\n",
    "\n",
    "    # scale the image in a way that maps its smaller dimension to the length of 300\n",
    "    wide = raw_pillow_image.width > raw_pillow_image.height\n",
    "    print(raw_pillow_image.width, \" \", raw_pillow_image.height)\n",
    "    if wide:\n",
    "        new_width = int(raw_pillow_image.width * 299 / raw_pillow_image.height)\n",
    "        new_height = 299\n",
    "    else:\n",
    "        new_width = 299\n",
    "        new_height = int(raw_pillow_image.height * 299 / raw_pillow_image.width)\n",
    "\n",
    "    # catually scale image \n",
    "    raw_pillow_image = raw_pillow_image.resize((new_width, new_height))\n",
    "    \n",
    "    # crop exceeding dimension\n",
    "    raw_pillow_image = raw_pillow_image.crop((0, 0, 299, 299))\n",
    "    \n",
    "    # scale to [0, 1]\n",
    "    pillow_image = (np.asarray(raw_pillow_image) / 255.0).astype(np.float32)\n",
    "    \n",
    "    # add dimension in the start (becaus3e we have only one image) and convert to tensor \n",
    "    tf_image = tf.multiply(tf.subtract(tf.expand_dims(pillow_image, 0), 0.5), 2.0)\n",
    "    \n",
    "    return pillow_image, tf_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500   300\n"
     ]
    }
   ],
   "source": [
    "img1_path = './ImageNet/467863966_a01cbd9d90.jpg'\n",
    "img2_path = './ImageNet/857502810_4313e2fbd4.jpg'\n",
    "img3_path = './ImageNet/81-yKbVND-L.png'\n",
    "img4_path = './ImageNet/81-yKbVND-L.jpg'\n",
    "img5_path = './ImageNet/EJxGm89VUAERo8e.jpg'\n",
    "img6_path = './ImageNet/flowers.jpg'\n",
    "\n",
    "my_raw_pillow_image = PIL.Image.open(img6_path)\n",
    "\n",
    "my_pillow_image, my_tf_image = preprocessing(my_raw_pillow_image)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.Variable(tf.zeros((299, 299, 3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_209AS",
   "language": "python",
   "name": "venv_209as"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
